{"title":"Implementation Plan","markdown":{"headingText":"Implementation Plan","containsRefs":false,"markdown":"\n## Objectives & Scope\n\nBuild a self-hosted chess tutor (\"chessmate\") that answers natural-language questions by combining structured metadata, semantic search, and LLM-based reasoning. The architecture evolves through three phases to balance simplicity, scalability, and agent-friendliness:\n\n1. **Phase 1 (MVP)**: Simple PostgreSQL filters + agent evaluation\n2. **Phase 2 (Scale)**: Hybrid search with vector pre-filtering + agent ranking\n3. **Phase 3 (Production)**: Pre-annotated tactical themes + deterministic filters\n\n**Core principle**: Design transparent, \"agent-friendly\" search tools that reasoning systems can understand and iteratively refine, rather than opaque ranking algorithms.\n\n## Current Status Snapshot\n\n| Milestone | Status | Notes |\n|-----------|--------|-------|\n| 1 â€“ Repository scaffolding | âœ… complete | Baseline structure, Alcotest smoke tests, CLI stubs. |\n| 2 â€“ Data ingestion foundations | âœ… complete | Real PGN parser, migrations, `chessmate ingest` wired to Postgres. |\n| 3 â€“ Embedding pipeline | âœ… complete | Jobs persisted, embedding worker loops, vector IDs recorded. |\n| 4 â€“ Hybrid query prototype | âœ… complete | Intent heuristics, `/query` API, CLI integration, ECO catalogue. |\n| 5 â€“ Simple agent-friendly search | ðŸš§ in progress | PostgreSQL filters + LLM agent evaluation (Phase 1 MVP). |\n| 6 â€“ Hybrid vector pre-filtering | ðŸ“‹ planned | Qdrant narrows candidates, agent ranks final results (Phase 2). |\n| 7 â€“ Theme pre-annotation | ðŸ“‹ planned | Tactical themes extracted during ingestion (Phase 3). |\n\n## Documentation Alignment Highlights\n\n- `README.md` highlights todayâ€™s feature surface: PGN ingestion, the curated hybrid query prototype, and the embedding worker skeleton that still targets OpenAI.\n- `RELEASE_NOTES.md` (v0.4.1) captures the ingest guard and parallel worker improvements while confirming the `/query` path still ships curated responsesâ€”Milestone 6 must close that gap by wiring live Postgres + Qdrant.\n- `docs/ARCHITECTURE.md` and `docs/CHESSMATE_FOR_DUMMIES.md` walk through ingestion â†’ embedding â†’ query flows and explicitly flag the current plannerâ€™s reliance on curated data.\n- `docs/DEVELOPER.md`, `CLAUDE.md`, and `AGENTS.md` lock in environment, tooling, and style guardrailsâ€”milestones should assume opam switch usage, two-space indentation, and `open! Base` everywhere.\n- `docs/OPERATIONS.md` and `docs/TROUBLESHOOTING.md` capture the smoke tests, health checks, and reset procedures that every milestone must keep healthy (e.g., quick Postgres connectivity check, embedding queue draining).\n- `docs/PROMPTS.md` already curates prompt templates for theme detection, pawn structure work, and future disambiguationâ€”reuse these instead of inventing new agent prompts.\n- `docs/GUIDELINES.md` reiterates the documentation-first culture; roadmap items should end with updates to the relevant handbook/runbook sections.\n\n**Implications for the roadmap**\n\n- Treat the curated `/query` responses as a documented limitation; Milestone 6 must close it by wiring the hybrid planner to live stores and updating docs to match.\n- Bake documentation/runbook edits into each milestone so README, developer, and operations guides stay in sync with behavior changes.\n- Reuse the shared prompt catalogue when implementing agent evaluation or theme extraction to preserve consistency across tooling and docs.\n\n## Phased Architecture Evolution\n\n### Phase 1: Simple + Agent-Friendly (Current Focus)\n\n**Design Philosophy**: Transparent, predictable search that agents can reason about.\n\n```mermaid\ngraph LR\n  User[User Query] --> Intent[Intent Parser]\n  Intent --> PG[(PostgreSQL)]\n  PG --> Filter[Structured Filters<br/>ECO, ratings, result]\n  Filter --> Candidates[Overfetch 50 games]\n  Candidates --> Agent[LLM Agent Evaluator]\n  Agent --> Results[Top 5 Results]\n```\n\n**Components**:\n- PostgreSQL: Deterministic filters (opening, ratings, result)\n- Intent Parser: Extract structured criteria + tactical keywords\n- LLM Agent: Reads game moves, evaluates tactical themes, ranks results\n\n**Example Query**: *\"Find King's Indian games where White executed a queenside majority attack, White â‰¥ 2500 ELO\"*\n\n**Flow**:\n1. Intent parser extracts: `opening=E60-E99`, `white_rating>=2500`, `theme=queenside_majority`\n2. PostgreSQL filters: `WHERE opening_slug='kings_indian_defense' AND white_rating>=2500` â†’ 50 candidates\n3. Agent evaluates: Reads move sequences, identifies queenside pawn majority patterns â†’ ranks top 5\n\n**Pros**: Simple, cheap, transparent to agents, good for MVP (<100K games)\n\n**Cons**: Agent evaluation costs tokens, slower for large candidate sets\n\n---\n\n### Phase 2: Hybrid with Vector Pre-Filtering (Scale)\n\n**Design Philosophy**: Use vector search as a **middle layer** to narrow candidates efficiently, then let agents reason about final selections.\n\n```mermaid\ngraph LR\n  User[User Query] --> Intent[Intent Parser]\n  Intent --> PG[(PostgreSQL)]\n  PG --> Filter1[1M games â†’ 10K<br/>ECO + ratings]\n  Filter1 --> Qdrant[(Qdrant)]\n  Qdrant --> Filter2[10K â†’ 100 positions<br/>vector similarity]\n  Filter2 --> Agent[Agent Ranking]\n  Agent --> Results[Top 5 Results]\n```\n\n**Components**:\n- PostgreSQL: Narrow by structured filters (10x reduction)\n- Qdrant: Semantic position similarity (100x reduction)\n- Agent: Final ranking and validation (100 â†’ 5 results)\n\n**When to Use**: Database > 100K games, unannotated PGN files, position-level pattern queries\n\n**Pros**: Scales to millions of games, finds patterns without annotations, position-level granularity\n\n**Cons**: More complex infrastructure, embedding costs, less transparent to agents\n\n---\n\n### Phase 3: Theme Pre-Annotation (Production)\n\n**Design Philosophy**: Pre-compute tactical themes during ingestion using LLM/rules, then use deterministic filters.\n\n```mermaid\ngraph LR\n  Ingestion[PGN Ingestion] --> Themes[Theme Extraction<br/>LLM/Rules]\n  Themes --> PG[(PostgreSQL<br/>+ theme tags)]\n  User[User Query] --> Intent[Intent Parser]\n  Intent --> PG\n  PG --> Filter[Deterministic Filters<br/>ECO + ratings + themes]\n  Filter --> Agent[Agent Refinement<br/>optional]\n  Agent --> Results[Top 5 Results]\n```\n\n**Components**:\n- Theme Extractor: Identifies tactical patterns during ingestion (one-time cost)\n- PostgreSQL: Stores pre-computed theme tags (e.g., `themes=['queenside_majority', 'king_attack']`)\n- Query: Deterministic filtering on theme tags\n\n**Example Tags**: `isolated_pawn`, `bishop_pair`, `rook_endgame`, `sacrifice`, `discovered_attack`, `pawn_storm`\n\n**Pros**: Fast queries, transparent filters, no agent evaluation needed for most queries, works offline\n\n**Cons**: Requires upfront theme extraction, may miss novel patterns\n\n---\n\n## Data Model Plan\n\n### Core Tables\n\n- **`games`**: PGN text, ECO, opening slug, event/site/date, players, ratings, result, `themes` (JSONB array)\n- **`players`**: name, aliases/FIDE IDs, rating peaks\n- **`positions`**: game_id, ply, FEN, SAN, eval, `vector_id`, `tactical_features` (JSONB), timestamps\n- **`embedding_jobs`**: status machine (`pending`, `in_progress`, `completed`, `failed`), FEN payload\n- **`themes`** (Phase 3): Tactical theme taxonomy with definitions for LLM extraction\n\n### Indexes\n\n- B-tree: `white_rating`, `black_rating`, `eco_code`, `opening_slug`, `played_on`\n- GIN: `themes` (JSONB array), `tactical_features`\n- Full-text: `pgn` (for keyword search in annotations)\n\n### Qdrant Payload (Phase 2)\n\n```json\n{\n  \"game_id\": 42,\n  \"white_name\": \"Kasparov\",\n  \"black_name\": \"Karpov\",\n  \"white_elo\": 2800,\n  \"black_elo\": 2750,\n  \"opening_slug\": \"kings_indian_defense\",\n  \"eco_code\": \"E97\",\n  \"ply\": 25,\n  \"themes\": [\"queenside_majority\", \"king_attack\"],\n  \"phase\": \"middlegame\"\n}\n```\n\n---\n\n## Library Structure (`lib/`)\n\n- **`chess/`**: PGN parser, game metadata, FEN engine, ECO catalogue, **theme taxonomy**\n- **`storage/`**: `Repo_postgres`, queue helpers, Qdrant adapter (Phase 2)\n- **`embedding/`**: OpenAI client, caching (planned), payload builders\n- **`query/`**: intent heuristics, **agent evaluator**, hybrid planner, result formatter\n- **`agents/`**: LLM client wrappers, prompt templates for theme detection and result ranking\n- **`cli/`**: ingest/query commands, shared env helpers\n\n---\n\n## Services & Workflows\n\n### Phase 1: Simple Agent-Friendly Query Flow\n\n```mermaid\nsequenceDiagram\n  participant User as User/CLI\n  participant API as Query API\n  participant Intent as Query Intent\n  participant PG as PostgreSQL\n  participant Agent as LLM Agent\n\n  User->>API: POST /query \"Find KID games with queenside majority\"\n  API->>Intent: analyse(question)\n  Intent->>PG: SELECT * WHERE opening='E60-E99' LIMIT 50\n  PG-->>Intent: 50 candidate games\n  Intent->>Agent: evaluate_tactics(candidates, theme='queenside_majority')\n  Agent-->>Intent: ranked results with explanations\n  Intent-->>API: top 5 games + agent reasoning\n  API-->>User: JSON response\n```\n\n### Phase 2: Hybrid Vector Pre-Filtering Flow\n\n```mermaid\nsequenceDiagram\n  participant User as User/CLI\n  participant API as Query API\n  participant Intent as Query Intent\n  participant PG as PostgreSQL\n  participant Qdrant as Qdrant\n  participant Agent as LLM Agent\n\n  User->>API: POST /query \"Similar to Kasparov-Topalov 1999\"\n  API->>Intent: analyse(question)\n  Intent->>PG: SELECT * WHERE white_rating>2600 (10K games)\n  PG-->>Intent: 10K candidates\n  Intent->>Qdrant: vector_search(position_embed, limit=100)\n  Qdrant-->>Intent: 100 similar positions\n  Intent->>Agent: rank_and_explain(positions)\n  Agent-->>Intent: top 5 with similarity reasoning\n  Intent-->>API: results + scores\n  API-->>User: JSON response\n```\n\n### Phase 3: Theme Pre-Annotation During Ingestion\n\n```mermaid\nsequenceDiagram\n  participant CLI as chessmate ingest\n  participant Parser as PGN Parser\n  participant Themes as Theme Extractor\n  participant Agent as LLM Theme Detector\n  participant PG as PostgreSQL\n\n  CLI->>Parser: parse(game.pgn)\n  Parser-->>CLI: moves + metadata\n  CLI->>Themes: extract_themes(moves)\n  Themes->>Agent: analyze_tactics(positions)\n  Agent-->>Themes: themes=['queenside_majority', 'rook_endgame']\n  Themes-->>CLI: enriched metadata\n  CLI->>PG: INSERT game + themes\n```\n\n---\n\n## Testing Strategy\n\n- **Unit tests** (Alcotest): PGN parser, metadata extraction, query intent heuristics, theme taxonomy\n- **Agent evaluation tests**: Prompt regression suite, theme detection accuracy metrics\n- **Integration tests** (Phase 2): Dockerized Postgres + Qdrant for ingest/query pipelines\n- **Regression suite**: Curated NL queries with expected results\n- **CI** (GitHub Actions): `dune build`, `dune runtest` on pushes/PRs\n\n---\n\n## Deployment & Operations\n\n### Local Development\n- Docker Compose: Postgres + Qdrant (optional in Phase 1), volumes under `data/`\n- Migrations: `./scripts/migrate.sh` (idempotent)\n- Environment: `DATABASE_URL`, `CHESSMATE_API_URL`, `OPENAI_API_KEY`\n\n### Production (Phase 3)\n- Containerized API + worker services\n- Theme extraction: Background job queue (similar to embedding worker)\n- Caching: Redis for frequent queries\n- Observability: Structured logs, Prometheus metrics, health endpoints\n- Security: TLS, API auth, least-privilege DB roles\n\n---\n\n## Milestones & Checkpoints\n\n### Milestone 1 â€“ Repository Scaffolding (âœ… Complete)\n**Objective**: Establish project skeleton, build system, initial tests.\n\n**Tasks**: Scaffold modules, add Alcotest smoke tests, set up CLI stubs, ensure `dune build`.\n\n**Checkpoints**: `dune build`, `dune fmt --check`, `dune test` pass; directory layout matches plan.\n\n---\n\n### Milestone 2 â€“ Data Ingestion Foundations (âœ… Complete)\n**Objective**: Parse PGNs, persist metadata, expose schema migrations.\n\n**Tasks**: Real PGN parser, migrations/seeds, build `chessmate ingest`.\n\n**Checkpoints**: Ingest sample PGN â†’ Postgres populated; `SELECT count(*) FROM positions` matches expectation.\n\n---\n\n### Milestone 3 â€“ Embedding Pipeline (âœ… Complete)\n**Objective**: Generate embeddings, sync Qdrant with Postgres (foundation for Phase 2).\n\n**Tasks**: Implement embedding client (retries/throttle), payload builder, `embedding_worker` service.\n\n**Checkpoints**: Docker stack brings up Postgres/Qdrant; worker inserts vectors; Postgres rows receive `vector_id`.\n\n---\n\n### Milestone 4 â€“ Hybrid Query Prototype (âœ… Complete)\n**Objective**: Answer NL questions via heuristic intent parsing.\n\n**Tasks**: Upgrade `Query_intent` heuristics (openings, ratings, keywords), add `/query` Opium API, wire `chessmate query` CLI, seed ECO catalogue.\n\n**Checkpoints**: CLI returns responses with filters; unit tests cover intent edge cases; docs updated.\n\n---\n\n### Milestone 5 â€“ Simple Agent-Friendly Search (ðŸš§ In Progress)\n**Objective**: Implement Phase 1 MVP with transparent PostgreSQL filters plus an OpenAI GPTâ€‘5 agent that ranks and explains results. All agent calls will use GPTâ€‘5 (Responses API) and the new `reasoning.effort` control.\n\n**Tasks**:\n1. **LLM Client & Config**\n   - Add `lib/agents/gpt5_client.ml/.mli` wrapping OpenAI GPTâ€‘5 with env-driven settings: `AGENT_API_KEY`, `AGENT_MODEL` (default `gpt-5`), `reasoning_effort` (`minimal|low|medium|high` default `medium`), `verbosity`, concurrency limit, retry/backoff.\n   - Remove temperature handling; expose knobs for `reasoning.effort` and verbosity as recommended in the GPTâ€‘5 docs.\n2. **Prompt & Response Schema**\n   - Reuse/extend motifs in `docs/PROMPTS.md` to craft prompts that summarise filters, send truncated PGNs, and request JSON output containing `score`, `themes`, and `explanation`.\n   - Implement robust response parsing with fallback when GPTâ€‘5 returns malformed JSON.\n3. **Data Fetching**\n   - Add `Repo_postgres.fetch_games_with_pgn ~ids` and, if needed, `Repo_postgres.fetch_positions ~ids` to supply the agent with sufficient context (limit to 50 candidates per query).\n4. **Pipeline Integration**\n   - Extend `Hybrid_executor` (or introduce `Agent_executor`) to: fetch metadata â†’ fetch PGNs â†’ evaluate with GPTâ€‘5 â†’ combine heuristic score and agent score (configurable weights) â†’ include agent explanation/effort metadata in results.\n   - Run agent evaluations concurrently with a bounded worker pool to respect rate limits.\n5. **Cost & Telemetry**\n   - Track tokens and latency per call; log structured telemetry (JSON) including `reasoning_effort`, tokens, cost estimates.\n   - Add optional caching (e.g., SQLite file or Redis key) keyed by plan+game to avoid re-billing identical prompts.\n6. **Testing**\n   - Unit tests for prompt generation and JSON response parsing (using canned GPTâ€‘5 outputs).\n   - Integration tests with a stubbed GPTâ€‘5 client to validate the full Postgres â†’ agent â†’ response loop.\n   - Update troubleshooting smoke test to cover agent-enabled queries and fallback behaviour.\n7. **Documentation & Runbooks**\n   - Update README, Operations, Troubleshooting with agent setup, `reasoning.effort` guidance, cost monitoring, and recovery steps when GPTâ€‘5 is unavailable.\n\n**Checkpoints**:\n- Query \"Find queenside majority attacks in King's Indian\" returns agent-ranked results with explanations (`reasoning.effort` tuned to `high`).\n- End-to-end latency < 10 seconds for 50 candidates with `reasoning.effort=medium`; document trade-offs for `low`/`high`.\n- Structured logs show tokens, effort level, and per-query cost; metrics captured in Ops runbook.\n- Integration tests cover agent path; documentation reflects new workflow and failure modes.\n\n**Architecture Note**: This milestone keeps reasoning transparentâ€”PostgreSQL filters remain deterministic while GPTâ€‘5 provides ranked insights with controllable depth via `reasoning.effort`.\n\n---\n\n### Milestone 6 â€“ Hybrid Vector Pre-Filtering (ðŸ“‹ Planned)\n**Objective**: Implement Phase 2 for scaleâ€”use Qdrant as middle layer, agents for final ranking.\n\n**Tasks**:\n1. Wire live Qdrant integration into hybrid planner\n2. Implement three-stage pipeline: PostgreSQL (10Kâ†’1K) â†’ Qdrant (1Kâ†’100) â†’ Agent (100â†’5)\n3. Add configuration for vector weight vs. agent weight in final scoring\n4. Build evaluation harness comparing Phase 1 (agent-only) vs. Phase 2 (hybrid)\n5. Optimize Qdrant payload filters to mirror PostgreSQL capabilities\n6. Instrument with metrics: query latency, candidate counts per stage, agent evaluation cost\n7. Revise README, `docs/ARCHITECTURE.md`, and `docs/CHESSMATE_FOR_DUMMIES.md` to describe the live hybrid wiring and update troubleshooting guidance for Qdrant failures\n\n**Checkpoints**:\n- Query against 1M+ positions completes in < 5 seconds (documented in `docs/OPERATIONS.md` as the new performance target)\n- Evaluation report shows recall/precision vs. Phase 1 baseline and is summarized in `RELEASE_NOTES.md`\n- Qdrant filters reduce candidates 100x before agent evaluation\n- Cost per query < $0.10 (OpenAI embeddings + agent evaluation)\n\n**Trade-off**: More complex infrastructure (Qdrant required) but scales to millions of games.\n\n---\n\n### Milestone 7 â€“ Theme Pre-Annotation (ðŸ“‹ Planned)\n**Objective**: Implement Phase 3 production architecture with pre-computed tactical themes.\n\n**Tasks**:\n1. Design theme taxonomy (20-30 common tactical patterns)\n2. Build `Theme_extractor` service that processes games during ingestion\n3. Create LLM prompts for theme detection from move sequences, aligning with examples in `docs/PROMPTS.md`\n4. Add `themes` JSONB column to `games` table with GIN index\n5. Migrate embedding worker pattern to theme extraction worker\n6. Update query pipeline to filter on pre-computed themes (no agent evaluation needed)\n7. Optional: Keep agent evaluation for edge cases (\"explain why this game matched\")\n8. Refresh `docs/OPERATIONS.md`, `docs/TROUBLESHOOTING.md`, and `docs/GUIDELINES.md` with the new ingestion workflow, QA checklist, and documentation expectations\n\n**Checkpoints**:\n- 95% of games have at least one theme tag after processing (summarized in `RELEASE_NOTES.md` with validation data)\n- Query with theme filter \"queenside_majority\" completes in < 100ms (PostgreSQL index scan)\n- Validation: Manual review of 100 games confirms theme accuracy > 85%\n- Cost: Theme extraction is one-time per game (vs. per-query agent evaluation); document operational costs in the runbook\n\n**Architecture Note**: This is the **production-ready** versionâ€”fast, deterministic, offline-capable, agent-friendly for debugging.\n\n---\n\n## Progress Log\n\n- **Milestone 1**: Baseline directory structure, modules, Alcotest smoke test. `dune build`/`dune test` green.\n- **Milestone 2**: PGN parser, migrations/seed scripts, `chessmate ingest` populates Postgres.\n- **Milestone 3**: Embedding jobs persisted, worker loops embedding FENs via OpenAI, vector IDs stored; `chessmate fen` diagnostic command added.\n- **Milestone 4**: Heuristic query planner, `/query` API prototype, CLI integration, ECO catalogue (`lib/chess/openings`), opening metadata persisted.\n- **Current (Milestone 5)**: Building agent-friendly search architectureâ€”PostgreSQL filters + LLM evaluation for transparent, iterative query refinement.\n\n---\n\n## Design Philosophy: Agent-Friendly Search\n\nBased on principles from [Reasoning Agents Need Bad Search](https://softwaredoug.com/blog/2025/09/22/reasoning-agents-need-bad-search):\n\n### Why Simple Tools Matter\n\n1. **Transparency**: Agents build \"mental models\" of search tools. Complex ranking algorithms are black boxes.\n2. **Iterative Refinement**: Simple filters allow agents to adjust queries based on initial results.\n3. **Predictable Behavior**: Deterministic filters (ECO codes, ratings) are easier to reason about than vector similarity scores.\n\n### Architecture Guidelines\n\n- **Phase 1 (MVP)**: Make PostgreSQL the primary tool. Let agents evaluate candidates.\n- **Phase 2 (Scale)**: Use Qdrant to *narrow* candidates, not replace reasoning. Agents still rank final results.\n- **Phase 3 (Production)**: Pre-compute what agents would compute at query time. Store it deterministically.\n\n### When to Use Each Approach\n\n| Query Type | Best Architecture | Rationale |\n|------------|------------------|-----------|\n| \"King's Indian games, White > 2600\" | Phase 1: PostgreSQL + agent | Structured filters handle 90% of work |\n| \"Games similar to Kasparov-Topalov 1999\" | Phase 2: Hybrid vector + agent | Pattern matching requires semantic similarity |\n| \"Queenside pawn majority attacks\" | Phase 3: Pre-annotated themes | Tactical concept, deterministically tag during ingestion |\n\n---\n\n## Future Enhancements\n\n### Beyond Phase 3\n\n1. **Multi-agent collaboration**: Specialist agents for openings, tactics, endgames\n2. **Interactive refinement**: Agent asks clarifying questions before searching\n3. **Explanation generation**: \"This game matched because moves 25-30 show...\"\n4. **Query templates**: Pre-built searches for common patterns\n5. **User feedback loop**: Learn from which results users actually view\n\n### Research Questions\n\n- Can we replace vector embeddings with LLM-generated feature descriptions?\n- How does agent evaluation quality scale with model size (GPT-4 vs. Claude Opus vs. GPT-5)?\n- What's the optimal balance between pre-computation (themes) and runtime reasoning (agents)?\n\n---\n\n## Related Documentation\n\n- [Chessmate for Dummies](CHESSMATE_FOR_DUMMIES.md) - Complete system explanation\n- [Architecture Overview](ARCHITECTURE.md) - System design and components\n- [Developer Handbook](DEVELOPER.md) - Contributing and coding standards\n- [LLM Prompts](PROMPTS.md) - Prompt templates for theme extraction and evaluation\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"IMPLEMENTATION_PLAN.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":["cosmo","brand"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}